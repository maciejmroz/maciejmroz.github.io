<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maciej Mróz Personal Blog</title>
    <link>/categories/product-management/index.xml</link>
    <description>Recent content on Maciej Mróz Personal Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/categories/product-management/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>LTV of a service</title>
      <link>/2014/11/04/ltv-of-a-service/</link>
      <pubDate>Tue, 04 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/11/04/ltv-of-a-service/</guid>
      <description>&lt;p&gt;A little story today. It goes like this. I drop by one of our teams, and within completely different discussion I get asked this very &amp;ldquo;simple&amp;rdquo; question:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Can we overlay an actual development team cost on top of product revenues so that we know if we are in the red or in the green?&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you haven&amp;rsquo;t noticed yet, there&amp;rsquo;s a flaw in this question. The problem is, it simply doesn&amp;rsquo;t make sense from economic point of view. For any project in any industry one thing is true: investment into project comes before realizing profit from that project. It&amp;rsquo;s pure logic, you just can&amp;rsquo;t defeat causality. Depending on the industry the distribution of investment and revenue over time axis will differ, but it might look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/investment-vs-revenue_300.jpg&#34; alt=&#34;investment vs revenue_300&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This offset between investment and revenue is the core of the problem: current development costs are &lt;strong&gt;always&lt;/strong&gt; out of sync with current revenue. Depending on the industry and project the gap will obviously vary but it&amp;rsquo;s always there. This is relatively simple and intiuitive concept when we talk about project-based work but it becomes somewhat more complex when we talk about online games, which are services. Some successful online games (like World of Warcraft) have been with us for a long time, but this is only because their developers keep working on them. In other words, the investment is continuous, and so is revenue (both curves do not have to be smooth!). These games wouldn&amp;rsquo;t have survived if they remained static in their entire life cycle, they evolved and changed together with their player base. In the world of game services, there&amp;rsquo;s huge overlap between the two curves from graph above but the truth remains: current investments create &lt;strong&gt;future&lt;/strong&gt; profits. Nothing new here, move along, and of post? Not really, this is where the fun actually begins.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s talk about something I decided to call &lt;strong&gt;service decay model&lt;/strong&gt;. What happens to the service once we completely stop development and leave only essential stuff: server operations, customer support, traffic acquisition? How long before it no longer makes sense to keep it running? If the product becomes static, it will continue to make money, only less and less every month. The decline does not happen overnight, and the tail is very long, but (with rare exceptions) we can safely assume that there&amp;rsquo;s some upper cap on total profits that we can extract from service in the period between now and before killing it completely at some unknown point in future. This upper cap is life time value (LTV) of a service. Let&amp;rsquo;s call it LTVS, I love abbreviations (on a more serious side, economists/financial folks probably have some more established name for this). We have no way to know LTVS, we can only try to build model that predicts it. Building predictive models is so hardcore math that Steve Jobs would just say it&amp;rsquo;s magical (you should build that model anyway, just because everyone sucks at predictions). What is interesting is that LTVS lets us define the value of current development: it is the &lt;strong&gt;change&lt;/strong&gt; it causes to LTVS. In other words: &lt;strong&gt;The feature is profitable if the increase in future revenue is bigger than the cost of implementing the feature.&lt;/strong&gt; To put it in pseudo equation form: Cost(feature) &amp;lt; delta LTVS. Why is that stuff important?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When you are early in the service life, it is &amp;ldquo;sink or swim&amp;rdquo;, it has to grow or you should just cut your losses and move on.&lt;/li&gt;
&lt;li&gt;When you are late in the service life, you should stop development way, way before you are in the red (in terms of service revenue vs development costs). When? When you no longer add value.&lt;/li&gt;
&lt;li&gt;If your service gets huge, economy of scale comes into play - 0.1% improvement might still be worth it if you have 10M users.&lt;/li&gt;
&lt;li&gt;Pulling the plug on service (killing it from end user perspective) may come long time after dev team moved on to other stuff.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By the way, this explains how larger companies came to the idea of separate product development (whose task is to deliver a product/service or make specific improvement/change in it) from operations (which is responsible for running the service day to day) and from marketing (which does user acquisition and community management). In theory it makes sense, but it also creates organizational silos and internal handoffs, along with numerous negative effects. You have been warned. An interesting (even if slightly bitter) thought to end this post is something I saw on Twitter few days ago: &amp;ldquo;There are two kinds of product features, those that improve product value, and those that justify someones job&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The rationale behind A/B testing</title>
      <link>/2013/02/24/the-rationale-behind-ab-testing/</link>
      <pubDate>Sun, 24 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/02/24/the-rationale-behind-ab-testing/</guid>
      <description>&lt;p&gt;A few days ago, while having a casual conversation over a beer, I was totally shocked when I heard something close to: &amp;ldquo;A/B testing is pointless on small project like ours because testing overhead is too high&amp;rdquo;. Is it? Let&amp;rsquo;s run the numbers :) Assumptions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The product costs X to operate monthly. 70% of the costs are development team, and 30% is administrative overhead and other costs like infrastructure, customer support and marketing expenses. The development cost obviously becomes much, much smaller percentage of the costs if the project is successful, but here we are taking about small project that still has to fight for success.&lt;/li&gt;
&lt;li&gt;Typical cost of implementing a feature is 0.5 month of development team work, so in our case it is 0.35*X.&lt;/li&gt;
&lt;li&gt;Averaged overhead of making new feature A/B testable is 10%. For a team that&amp;rsquo;s used to A/B testing stuff this is likely accurate number, for team that is not routinely doing it the overhead may be a lot higher, at least initially. If a feature that has base cost of 0.35*X, the A/B testing overhead is 0.035*X.&lt;/li&gt;
&lt;li&gt;Our alternative to A/B testing is picking the variant manually by product owner intuition. I assume absolutely great PO is right 80% of the time.&lt;/li&gt;
&lt;li&gt;Positive change brings the revenue up by 3%, negative change (a mistake) makes the revenue go down 2%.&lt;/li&gt;
&lt;li&gt;The product makes X per month, essentially it&amp;rsquo;s on the edge of getting axed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The question: should we A/B test changes in this scenario? Now something very important, and something people tend to overlook a lot: &lt;strong&gt;anything&lt;/strong&gt; you do with the product impacts revenue over a long time. It&amp;rsquo;s not just one month, but possibly event years when the change is active. In order to properly think about A/B testing, we have to take this into account. Let&amp;rsquo;s take 6 month period:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Not running A/B test gives us expected payoff of (0.98*0.2+1.03*0.8)*6*X, which is 6.12*X.&lt;/li&gt;
&lt;li&gt;Running A/B test means we are almost always right, and gives us payoff of 1.03*6*X, which is 6.18*X.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We made 0.025*X (0.06 minus cost of A/B test) by running the A/B test! Note that the model I used to illustrate the need for A/B testing is highly simplified. For example, in reality there&amp;rsquo;s is some cost associated with delay introduced by testing, the product performance while tests are running is a mix of good and bad variant, the variants between which we are trying to decide may both be good, both bad, or one may be neutral etc etc. The result of this added complexity is not to abandon A/B testing! The goal is not to break even but to make the product make 10 times as much! Bear in mind that any mistake you make early in the product lifecycle will impact your future revenue. The model above shows A/B testing is profitable even if you play not to lose. If you play to win, A/B testing is something you just &lt;strong&gt;have to&lt;/strong&gt; be doing.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>