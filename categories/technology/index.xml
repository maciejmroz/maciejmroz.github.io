<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maciej Mróz Personal Blog</title>
    <link>https://www.maciejmroz.com/categories/technology/index.xml</link>
    <description>Recent content on Maciej Mróz Personal Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://www.maciejmroz.com/categories/technology/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Beyond virtualization</title>
      <link>https://www.maciejmroz.com/2015/01/01/beyond-virtualization/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2015/01/01/beyond-virtualization/</guid>
      <description>&lt;p&gt;I guess it&amp;rsquo;s time to sum up recent trends in how we build, deploy, and operate complex server side software. Virtualization and cloud computing have been with us for quite a while but right now the era of containers is coming, with entire Docker ecosystem paving the way. You might want to ask a very valid question: &amp;ldquo;What&amp;rsquo;s in it for me?&amp;rdquo; :) I will focus on business implications of the technology. But let&amp;rsquo;s set the context first.&lt;/p&gt;

&lt;p&gt;The explosion of cloud computing in recent years literally changed the way we think. Well, you probably could treat Amazon EC2 instances as if they were ordinary machines, but &amp;ldquo;in the cloud&amp;rdquo;. That of course would be &lt;strong&gt;totally&lt;/strong&gt; missing the point. The key advantages of cloud computing are on the intersection of operations, software architecture and development - these cannot be realized without moving to true DevOps. Trying to slap &amp;ldquo;cloud computing&amp;rdquo; sticker on top of existing organization with IT/Operations/Development (add more if needed to fit your case) silos will not give anyone much value. Once you change your approach, you suddenly get the true benefits:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shorter cycle time from concept to delivering customer value&lt;/li&gt;
&lt;li&gt;Ability to to a lot more ad-hoc without unnecessary risk&lt;/li&gt;
&lt;li&gt;Resilient and scalable application architecture - one that&amp;rsquo;s flexible and evolving, not set in stone&lt;/li&gt;
&lt;li&gt;Reduced TCO thanks to automation, completely new usage patterns (i.e. instance that runs once a day for an hour) and services that cloud computing provider can operate way more efficiently than everyone else&lt;/li&gt;
&lt;li&gt;If everything else fails, ability to brute force your way out of many problems :)
The real cloud computing usage scenarios could look like this:&lt;/li&gt;
&lt;li&gt;The developer spins up new game instance because the team anticipates that the changes deployed just minutes ago will require more processing power. It happens in less than five minutes, instance is automatically put in proper security group, attached to load balancer (if necessary), tagged, monitoring is set up so that any problems with the instance are escalated to the team that launched it, and serving traffic! By the way, instance is based on a machine image that was generated automatically by Jenkins job triggered in the morning by change to the Git repository - few hours earlier someone found and fixed race condition in the gaming platform. All with single click or call of a command line.&lt;/li&gt;
&lt;li&gt;An analyst spins up an database instance with the last snapshot of production data, runs some very expensive SQL query, then shuts it down. All happening without any disruption to production service, at very low infrastructure cost, without bothering anyone. CEO gets the report he ordered a lot quicker than expected :)&lt;/li&gt;
&lt;li&gt;We get huge traffic spike on some of our smaller apps - this time somebody on the team actually makes decision to move to bigger instance type and spins up 3 larger instances, waits till they start serving traffic, then stops 4 smaller ones. Because of underlying automation, the new instances are automatically spread across availability zones. All this happens without any disruption to the service.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these scenarios, and a lot more, could happen at my company. In fact, they happen all the time, sometimes in parallel. To us these are just events. Not issues, emergencies, or disasters, but events. Extensive planning, overprovisioning, delays, service disruptions &amp;hellip; with the right mindset, you no longer have to worry about these and can focus on solving other problems. At least most of the time, obviously it&amp;rsquo;s not always perfect and rosy :) Bottom line: once you truly embrace cloud computing, the value is huge. If we can do all this stuff, why are people so exctited about using containers instead of just spinning up VMs like we do now?  Here we come to the hard part, because if you are a software developer who used a public or private cloud the way it should be used, you just know that Docker is a huge deal. If you didn&amp;rsquo;t, you might have trouble here but I&amp;rsquo;ll try to explain anyway.&lt;/p&gt;

&lt;p&gt;What public IaaS/PaaS services have really given us was a glimpse into the future. At the same time, they only took us half the way. With AWS etc, people have started to build resilient and scalable systems, but many wanted to take it further. Microservices are an obvious step but even cheapest and smallest VMs out there are just too powerful for that approach. If you imagine an app that&amp;rsquo;s deployed on 100+ micro instances for architectural reasons, well, that&amp;rsquo;s not pretty. We went to the cloud to reduce operational overhead, yet we found ourselves under pressure to actually increase it and to waste computing resources in the process. Containers enable reasonable isolation without overhead of using a VM. Starting a container is not really different from starting a process (in terms of both resource usage and speed!). Docker makes all that easy to use, and easy to build further automation on top of. All the missing pieces are suddenly in place. We get to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Architecture described in code, possibly changing from version to version, deployed and scaled automatically&lt;/li&gt;
&lt;li&gt;Heterogeneous environment that doesn&amp;rsquo;t dissolve into complete mess&lt;/li&gt;
&lt;li&gt;Continuous delivery of user value with even lower risk - deployment at service level, very easy side by side deployments&lt;/li&gt;
&lt;li&gt;Modularization enabling everyone to better control technology debt, and giving the option of rewriting stuff if absolutely necessary&lt;/li&gt;
&lt;li&gt;Standardized workflow: developer machine, test, staging, production - all running the same code, just configured differently&lt;/li&gt;
&lt;li&gt;All that without locking ourselves into using single IaaS/PaaS vendor (I don&amp;rsquo;t think of it as a big deal, but some people do)&lt;/li&gt;
&lt;li&gt;With close to optimal resource usage because we can pack many containers per VM!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What does it all mean if I&amp;rsquo;m just starting? When I was finishing my studies, early into professional career, the cheapest way to set up a server was to use bare metal machine and pay $300/mo or something like that, very often on 12-months+ contract. And these servers weren&amp;rsquo;t really that powerful, iirc it was something like dual socket single core Xeon (several generations behind!) + 1 GB of memory and small SCSI disk. You want RAID? Load balancer and more machines for better availability or to scale performance? Dedicated DB machine? Costs were going up very, very quickly. Count in operational overhead of bare metal hw and you ended up with pretty high numbers (account for the fact that back then there was no Puppet/Chef/Ansible etc - we used bash and Perl to automate operations!).&lt;/p&gt;

&lt;p&gt;Today you can get a fully featured VM for as little as $5/mo, so you can easily host replicated db + redundant web instances for $20/mo. You can scale it up when the needs arise, with provisioning within minutes. Thanks to Docker, you can use this setup to build using microservices from the very beginning, so there will be no need to go beyond these cheap VMs before you are actually resource limited. Realistically, we are talking 30-50x reduction in initial infrastructure costs. On operational side everything is also a lot easier thanks to APIs and much more mature tools. Setting up/tearing down test environments can be fully automated. If you go with AWS, for a slightly higher price you can get DB and storage services that require almost zero work on your side (for that reason AWS probably is the sweet spot when you are limited by time). What has also changed is the efficiency of our infrastructure usage - LAMP stack is pretty much dead now (except maybe for the DB part) and people moved towards things like node.js, Go, or some new JVM based stuff which bring further cost savings. Not only is the infrastructure cheaper, we can also get more from it, and all that without sacrificing application architecture. Obviously, you still have to know how to build distributed applications, but having that knowledge things are a lot easier now :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The structure in unstructured data</title>
      <link>https://www.maciejmroz.com/2013/03/09/the-structure-in-unstructured-data/</link>
      <pubDate>Sat, 09 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2013/03/09/the-structure-in-unstructured-data/</guid>
      <description>&lt;p&gt;Working with data is something I personally consider one of the coolest things one can do right now in the tech industry. Typically when someone thinks or talks about &amp;ldquo;Big Data&amp;rdquo; it implies petabyte data sets and Hadoop clusters spanning 100+ nodes. The truth is, even a lot smaller data and humbler infrastructure can provide great insights and drive the product/service innovation. To me its really a lot closer to &amp;ldquo;Smart Data&amp;rdquo; than &amp;ldquo;Big Data&amp;rdquo; - it doesn&amp;rsquo;t matter how big your data set is, but how much you are able to do with it, and how quickly.&lt;/p&gt;

&lt;p&gt;In my workplace we run our own data gathering, storage, and processing. Our own code, our own infrastructure. Managed, architected, and even partially implemented by yours truly. What is special about our system is that compared to most commercial systems, we allow our products to report pretty much arbitrary data. There are very basic guidelines that must be met for standard metrics to work, but what is actually possible goes way beyond that. Product teams have access to raw data and can create their own processing tools if they want/need to. They can also use Hive to query the data if they want to - very often you can just brute force your answer from data, so why not do it? Programmer&amp;rsquo;s time is expensive, after all.&lt;/p&gt;

&lt;p&gt;The problem is that if you are not willing to throw a lot money at scaling your Hadoop installation, the queries start to become very slow, very quickly. And that really kills the organization agility: having to wait many hours for query result is not what anyone wants. The cycle from running set of queries to releasing new version of the game (or whatever the organization is doing) should optimally be possible to fit within one business day. Running single Hive query for many hours prevents that. It is a problem I consider to be strategically critical, so I decided to attack it myself.&lt;/p&gt;

&lt;p&gt;Because my target was reducing query times from hours to minutes, I needed solution that&amp;rsquo;s radically different from simply getting more hardware. I needed to get smarter about how the data is processed. A word of warning: if you know what &lt;a href=&#34;http://en.wikipedia.org/wiki/OLAP_cube&#34;&gt;OLAP cube&lt;/a&gt; is, and explored how OLAP systems work, what I am about to say will likely sound very basic. You may stop reading now :) The first time I&amp;rsquo;ve been able to play with a system like that, I wasn&amp;rsquo;t really impressed by what it can do, but I was deeply impressed by how quickly it was coming up with responses. Seriously, if a system can answer your question about data set faster than it takes to read it from disk there&amp;rsquo;s something quite sophisticated happening behind the scenes. Obviously, they couldn&amp;rsquo;t just index the data for queries I was about to ask without knowing the queries. Classic btree indexes were not the answer. But it was still very quick. To understand why one needs to realize very simple thing: yes you can slice&amp;dice; through multidimensional data very quickly, but at the same time you are limited to quite simple queries. The data is preprocessed and stored in a way that makes specific type of queries run fast. Vast majority of analytical queries will fall into category of exploring the OLAP cube.&lt;/p&gt;

&lt;p&gt;My first thought about the problem of preproprocessing the data was that we cannot do it because it has no fixed structure. It&amp;rsquo;s all JSON objects, the product teams can put anything they want inside, how am I going to do anything meningful with it? 100 milliseconds later I realized that calling our data unstructured is just not true. Yes, it would be hard to fit into fixed schema SQL table, but it has tons of regularities. In fact, our technical guidelines for analytics instrumentation specify recommended list of dimensions, we just call them differently! And, because I typically acted as an outside expert to the team when planning the instrumentation, the guidelines are actually followed!&lt;/p&gt;

&lt;p&gt;Realizing that our data has a lot of structure was the tipping point. The idea looked good, I started some Python programming in order to implement a system that would process the raw data and store the aggregated cache for later processing. I decided to cut out some less important stuff (very noisy dimensions), and obviously cut out data that makes little sense aggregating. Coding, coding, coding &amp;hellip; done. Ok, perhaps it was slightly more complicated. For example, some dimensions were not known before processing and I had to use a bit of regexp magic :) Still, nothing I would really call hard. After launching the aggregator script I was very pleased with the results: depending on the project, I was getting 50-100x smaller data size compared to raw data set. That pretty much linearly translates into shorter query times. While the preprocessing is rather costly, it happens only once per data chunk so it&amp;rsquo;s not really an issue - just another cron job to run on one of servers (or more, should the need arise :P ). It&amp;rsquo;s not a solution to fit every single use case out there. In fact, I think this solution is just another great example of Pareto principle - I solved 80% of the problems with 20% of effort. What&amp;rsquo;s good is that now we have more time and computing resources to focus on the remaining 20% of data problems, which are a lot tougher than quickly generating simple reports :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is virtualization a failure of operating systems?</title>
      <link>https://www.maciejmroz.com/2012/12/19/is-virtualization-a-failure-of-operating-systems/</link>
      <pubDate>Wed, 19 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2012/12/19/is-virtualization-a-failure-of-operating-systems/</guid>
      <description>&lt;p&gt;One thing has hit me while watching of this year&amp;rsquo;s LinuxCon Europe presentations - one of reasons virtualization exists in the first place is operating system inability to properly isolate the applications from each other. Look at it this way: the role of operating systems is abstracting the physical hardware from the applications. And what is the role of hypervisor? Abstracting the hardware from the opertaing system(s) … Which makes the operating system an application running under the hypervisor, doesn&amp;rsquo;t it?&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s think of it from other angle - why are we using virtualization? We are virtualizing applications, and operating system is only a required intermediary. Typical workload of virtual machine is exactly one application (in general case this is one-to-many relationship, because often single application spans multiple VMs). We do this partly to achieve good utilization of physical hardware, but also to achieve isolation at the level close to &amp;ldquo;physical box per every application&amp;rdquo;. Not as secure, but conceptually equivalent.&lt;/p&gt;

&lt;p&gt;In fact, this level of isolation is desireable on the desktop, too. VM to run untrusted USB stick? Seems like a good idea, and that&amp;rsquo;s only one of possible use cases. If we are running entire operating system just to run one application, isn&amp;rsquo;t it a waste? What can run under the hypervisor doesn&amp;rsquo;t really have to be a complete operating system. There are projects like Erlang on Xen (&lt;a href=&#34;http://erlangonxen.org&#34;&gt;erlangonxen.org&lt;/a&gt;) or Mirage that seem to skip the OS and talk straight to the hypervisor API - I am not familiar with the implementation details of these projects, but that seems to be general idea. And that makes hypervisor … an operating system. An operating system that opens up plethora of new and exciting possibilities, and one much better fit for todays world - but still an operating system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Enter the Node.js</title>
      <link>https://www.maciejmroz.com/2012/07/07/enter-the-node-js/</link>
      <pubDate>Sat, 07 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2012/07/07/enter-the-node-js/</guid>
      <description>

&lt;p&gt;The title says it all, but I probably still should paint some background to this article. Imagine a system where you need very high performance, you need it yesterday, and you don&amp;rsquo;t have infinite funds to simply throw more hardware at the problem. Very important component in this system is a web server. Very simple requests, always generated by code (while system in question does not provide data to end users, you can think of AJAX - definitely there&amp;rsquo;s a similarity). So I have a webserver in question. Apache + PHP, running on a virtual machine. I quickly written a Python script that emulated the workload webserver was supposed to have (fuzzed POST requests in specific format). I did back of the envelope calculations of the performance the web server has to meet. As it turned out, Apache + PHP running on a virtual machine was at very, very small percentage of what I needed.&lt;/p&gt;

&lt;p&gt;Obvious thing I tried was to get a VM with a lot more CPU power. It indeed got me a lot more performance, but as you might have guessed already, getting the improvement I needed was hardly possible this way. So I moved the webserver to a physical box (I have these too :) ). As I quickly learned after going physical, the Python script I was using to test the webserver couldn&amp;rsquo;t truly cut it any more and was adding a lot of overhead. I gave up the idea of precisely simulating the workload, took one example request, and used the industry standard - Apache Benchmark (ab). Even before that is was obvious that Apache + PHP is not going to cut it. My initial idea for improvement was to use nginx + PHP running in FastCGI mode. It did run, and initial results were quite promising but &amp;hellip; you&amp;rsquo;ll see results below. Anyway, nginx wasn&amp;rsquo;t cutting it.&lt;/p&gt;

&lt;p&gt;I intended to take a deeper look at node.js for a long time but never really had a project that would push me far enough. This one was it. I am not a web developer, and JavaScript is hardly my language (I think in C++ but usually use Python because I am lazy). Anyway, imperative languages are mostly the same, I hacked node.js app that did what I needed. I slightly changed system design along along the way - node.js output format was JSON instead of CSV used by PHP app. Still, the apps were computationally equivalent, and it terms of I/O node.js was actually a bit more verbose. Output was pushed over the network to another service, so web app itself was not doing any disk I/O. Backend service was not the bottleneck.&lt;/p&gt;

&lt;p&gt;The benchmark was done on Core i3 540, CentOS Linux 6.2, using newest node.js, PHP 5.3.x with APC installed, newest stable nginx, Apache 2.2.x. Node app was using &amp;lsquo;cluster&amp;rsquo; module (built in) in order to use all available logical CPU cores. Nginx was using PHP via FastCGI interface, Apache was running in &amp;lsquo;prefork&amp;rsquo; mode and PHP compiled as a module. I tried to benchmark 8,32,128,512,1024, 2048, 4096 simultaneous clients on 500k requests. Machine used to run ab was using some quad core Intel CPU and CentOS Linux 5.5 and was connected to the same 1 GBit switch the web server was connected to.&lt;/p&gt;

&lt;h2 id=&#34;apache-vs-nginx&#34;&gt;Apache vs Nginx&lt;/h2&gt;

&lt;p&gt;I guess that before going in details into results that I got from node it&amp;rsquo;s good to compare Apache and nginx - there&amp;rsquo;s a lot of comparisons out there, and many of them suggest that nginx is significantly better &amp;hellip; Apache results:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;8 clients: 2250 requests/sec, 4 ms latency&lt;/li&gt;
&lt;li&gt;32 clients: 2250 requests/sec, 14 ms latency&lt;/li&gt;
&lt;li&gt;128 clients: 2100 requests/sec, 60 ms latency&lt;/li&gt;
&lt;li&gt;512 clients: 2100 requests/sec, 240 ms latency&lt;/li&gt;
&lt;li&gt;1024 clients: FAILED&lt;/li&gt;
&lt;li&gt;2048 clients: &amp;mdash;&amp;mdash;-&lt;/li&gt;
&lt;li&gt;4096 clients: &amp;mdash;&amp;mdash;-&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apache had mostly flat performance, going slightly down from 32 to 128 clients. At 1024 clients it simply stopped accepting connections. Still, if the request completed, it completed without error, and up to 512 simultaneous clients request latency was going up linearly with number of clients. Honestly, these are not bad results and I&amp;rsquo;d be fine with them if my target wasn&amp;rsquo;t at 5k requests per second. Nginx results:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;8 clients: 2250 rps, 4 ms latency&lt;/li&gt;
&lt;li&gt;32 clients: 2350 rps, 14 ms latency&lt;/li&gt;
&lt;li&gt;128 clients: 2350 rps, 55 ms latency&lt;/li&gt;
&lt;li&gt;512 clients: 2100 rps, 241 ms latency (~500 failed!)&lt;/li&gt;
&lt;li&gt;1024 clients: 1900 rps, 540 ms latency (~3650 failed!)&lt;/li&gt;
&lt;li&gt;2048 clients: FAILED (completed but error rate 80%+)&lt;/li&gt;
&lt;li&gt;4096 clients: &amp;mdash;&amp;mdash;-&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nginx was never vastly faster than Apache. Still, at realistic workloads it indeed was slightly faster. What happened at higher concurrency levels was interesting. At 512 clients roughly 500 requests failed (completed with HTTP code other than 2xx) which is about 1 in 1000. The error rate went up even more at 1024 clients. The breakdown happened at 2048 clients - the benchmark did complete, but with more than 80% of requests ending in non-2xx code (I suppose it was 500 Internal server Error or something like that &amp;hellip;). To sum things up: the way nginx performance degrades with concurrency is different from Apache, and on the extreme end nginx was actually slower. Apache and nginx, while different, were still in the same performance league. I still needed 5000 requests per second and it was time to try out node.js.&lt;/p&gt;

&lt;h2 id=&#34;welcome-the-king&#34;&gt;Welcome the king&lt;/h2&gt;

&lt;p&gt;Node.js results:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;8 clients: 4700 rps, 2 ms latency&lt;/li&gt;
&lt;li&gt;32 clients: 6900 rps, 5 ms latency&lt;/li&gt;
&lt;li&gt;128 clients: 7200 rps, 18 ms latency&lt;/li&gt;
&lt;li&gt;512 clients: 7200 rps, 70 ms latency&lt;/li&gt;
&lt;li&gt;1024 clients: 7150 rps, 143 ms latency&lt;/li&gt;
&lt;li&gt;2048 clients: 7100 rps, 288 ms latency&lt;/li&gt;
&lt;li&gt;4096 clients: 6900 rps, 590 ms latency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Shocking, isn&amp;rsquo;t it? Node was running in circles around traditional web servers! Its peak performance was not only at higher concurrency level, but the performance drop with more clients was only slight, instead of completely dying. It was able to complete the benchmark up to 4096 simultaneous clients, with 0 (zero!) errors (even if latency was high) &amp;hellip; For me it&amp;rsquo;s almost end of the road - I got more performance than I actually needed. Now it&amp;rsquo;s time to polish the code, write documentation etc - and then move to other parts of the system.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d love to repeat the benchmark on faster CPU some day (the i3 used was all I had in the office at the moment) - it seems that all servers in question would benefit a lot from more CPU power. Also, redoing the benchmarks on faster CPU with more physical cores might reveal some other limitations. After this experience, node is definitely going to be important part of my toolbox - it seems perfectly suited for many of the things I am struggling with at work. Still, it is unlikely to ever replace traditional webservers across the stack.&lt;/p&gt;

&lt;p&gt;Something to think about: HTTP is actually very simple protocol. We made it complicated, and mainstream webservers moved along to support all the bells and whistles. What we have right now is like showing up on a car race with expensive limousine: it may have tons of great gizmos and might be a better all-rounder, but on a race track it is crushed by cars that were built to race.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The demise of spinning disks</title>
      <link>https://www.maciejmroz.com/2012/03/18/the-demise-of-spinning-disks/</link>
      <pubDate>Sun, 18 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2012/03/18/the-demise-of-spinning-disks/</guid>
      <description>&lt;p&gt;It was quite some time ago when my company started using Fusion-IO (enterprise SSDs) on our database servers. We are happily using them till this day. The great part is that we could scale vertically and leave software layer unchanged. Even better: we &lt;strong&gt;still&lt;/strong&gt; have room for growth, despite getting more users and our application stack getting more and more demanding. We were one of the first to switch to SSDs on server side - so right now it&amp;rsquo;s interesting that this idea propagated so quickly. We got to the point where people start loudly saying &amp;ldquo;disk is the new tape&amp;rdquo;. And it really is.&lt;/p&gt;

&lt;p&gt;The gap between CPU speed and external memory has been widening for a long time, and got to the point of being simply ridiculous. It&amp;rsquo;s worth noting that it&amp;rsquo;s not only about gap between CPU and disk drive but also about gap between CPU and DRAM (which is slightly less known fact). For CPU accessing L1 cache takes single digit amount of clock cycles. Subsequent levels of memory hierarchy are slower and slower. Accessing main memory gets us into area of hundereds to even 1000+ CPU cycles (this is strongly architecture dependent). This is very bad and low level programmers have been battling this issue for a long time now. One of approaches is using access patterns that minimize CPU cache misses and using low level prefetch instructions. We are essentally talking about linearizing access to memory that is supposed to be &amp;ldquo;random access&amp;rdquo;. If latency is the problem in case of DRAM, how bad it is in case of disk drives? Very, very bad.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with simple analogy. I need something and:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it happens to be in my room (that&amp;rsquo;s my L1 cache) - it takes a few seconds to get what I need&lt;/li&gt;
&lt;li&gt;ok, it&amp;rsquo;s not on my desk (I have to hit DRAM) - it takes a few minutes to get what I need, an equivalent to, say, walking to a neighbor and borrowing something&lt;/li&gt;
&lt;li&gt;hmmm, actually my neighbor doesn&amp;rsquo;t have it and I have to hit the store - how much would it take if the store was the disk?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is where 99% of people totally fail at estimation saying something like &amp;ldquo;few hours&amp;rdquo;. The real answer is: much, much longer.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume that DRAM latency is at 100 ns (so about 300 clock cycles of 3GHz CPU), and average disk seek time is 6 ms (a very fast disk!). The difference is 60 000 times! We are not talking about &amp;ldquo;get into a car and drive to the store&amp;rdquo;. In our hypothetical scenario we are actually talking about &lt;strong&gt;months&lt;/strong&gt;! But it&amp;rsquo;s only part of the story, because it&amp;rsquo;s not just about latency - it&amp;rsquo;s also about parallelism.&lt;/p&gt;

&lt;p&gt;For a hard disk drive latency determines the upper number of random I/O operations per second because disk has to do everything serially. The serial nature of HDDs is the real problem and source of pain as CPUs are becoming more and more parallel, putting more and more processing power into roughly the same space. Because CPU throughput is skyrocketing (high end Xeon E7 is 10 cores/20 threads), we need more and more I/O operations per second. Single hard disk cannot provide these, and will never be able to :( In fact, most of server side programming nowadays revolves around optimizing for I/O (in general), not for CPU. This happens at many levels - operating system kernel, file systems, database storage engines, applications &amp;hellip; Tons of software engineering work was performed just to hide simple fact that the disks cannot keep up. Interesting consequence is the proliferation of Hadoop (and other implementations of MapReduce) as an alternative to relational databases in many data processing tasks. Hadoop takes what&amp;rsquo;s good about disks (cheap bandwidth) and gets rid of the latency problem by streaming all the data, instead of randomly accessing it.&lt;/p&gt;

&lt;p&gt;SSDs still have latency of ~0.1 ms - it a lot faster than HDDs, but at the same time a lot slower than DRAM &amp;hellip; What&amp;rsquo;s special about SSDs is not the access time - it&amp;rsquo;s the parallelism. Enterprise SSDs offer hundreds of thousands of I/O operations per second - the difference here is a lot bigger than the difference in access times. Additionally, while the performance of disk drives is not really improving any more, Flash memory based solutions have huge room for improvement - and their I/O performance is going to improve as fast or even faster than CPU performance. One very simple reason for that is that right now we are still trying to access them through traditional file I/O interfaces that weren&amp;rsquo;t meant for this kind of usage &amp;hellip; In fact, when we start treating SSDs for what they really are, a memory, and forget about block device+filesystem nonsense, we are going to see massive performance gains. It&amp;rsquo;s already happening in the form of Auto Commit Memory technology from Fusion-IO. Surely more will follow, because right now interfaces and API layers are huge source of inefficiency. Also, it&amp;rsquo;s good to know that NAND Flash is not the last word in technologies of non-volatile memory and there are others on the horizon (although I am not aware of any commercial applications right now). We are heading towards a world where speed gap between HDD and SSD is measured in thousands of times, and getting wider every day.&lt;/p&gt;

&lt;p&gt;Unfortunately, because it&amp;rsquo;s a revolutionary shift, the price of Flash memory does not go down as fast as one might want it to. Simply speaking, now &lt;strong&gt;everyone&lt;/strong&gt; wants all of their hot data to be in Flash memory. Massive demand for Flash memory in consumer electronics does not make things any better - Apple is likely sucking in significant chunk of world production :) Interesting question is: where are the SSDs in the cloud? Recently, as it turns out, Amazon used a lot of these to launch DynamoDB. Which from my point of view creates interesting problem - because it becomes essentially impossible to build comparable service on &amp;ldquo;raw&amp;rdquo; EC2. That means getting locked into AWS. Will Amazon introduce some sort of &amp;ldquo;high I/O EBS&amp;rdquo;? I am sure I am not the only one interested in such product.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>If you haven&#39;t noticed, OpenGL API is alive and kicking</title>
      <link>https://www.maciejmroz.com/2012/03/04/if-you-havent-noticed-opengl-api-is-alive-and-kicking/</link>
      <pubDate>Sun, 04 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2012/03/04/if-you-havent-noticed-opengl-api-is-alive-and-kicking/</guid>
      <description>&lt;p&gt;Recently I somehow found enough time to read &amp;ldquo;OpenGL SuperBible, 5th edition&amp;rdquo;. It was something done purely for fun, just to see how things are looking today for modern graphics APIs. Last time I did any serious graphics programming, OpenGL was slowly dying.&lt;/p&gt;

&lt;p&gt;In fact, my M.Sc. thesis used Direct3D as implementation API. I simply had no choice. There was no standard for anything - every single important feature was exposed through extensions. At the same time, Direct3D 9 was well thought, simple, and forward looking. Microsoft was leading the way, and everyone knew that. As an example: somewhere around 2005, EXT_framebuffer_object appeared, giving GL functionality that was available since Direct3D 7! Good news: things have changed, and they have changed for better.&lt;/p&gt;

&lt;p&gt;In fact, todays OpenGL is awesome. And that&amp;rsquo;s despite Microsoft API advancing two versions forward since then! Extension hell is gone - core OpenGL defines 99.9% of the stuff you need (I think that the only universally supported functionality that is not in the core is anisotropic filtering and S3TC formats). And there is such thing as &amp;ldquo;core profile&amp;rdquo;, making OpenGL shader-only API, with all the legacy stuff removed. What&amp;rsquo;s also great is that GL went the way of D3D and states pretty clear minimum requirements to support any feature: texture units, inputs and outputs of specific shader stages, texture and renderbuffer formats etc. These minimums are set quite high, and generally correspond to what you see in D3D. Now, I am sure thare are still some differences between the two APIs, but I believe they are pretty much irrelevant.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go back to the book, because by that time you might be wondering &amp;ldquo;is it any good?&amp;rdquo;. The reviews on Amazon aren&amp;rsquo;t really favorable but my personal opinion is that it is a very, very good book to teach the API and get yourself started with modern GPU programming. Author does not waste time describing legacy fixed function functionality, which is a big plus. You might wonder how to achieve that without falling back to specification-like language - in shader-only environment there&amp;rsquo;s a lot you have to do before anything actually appears on screen: write, compile and link your shaders (even if simplistic ones), upload geometry data to the GPU, activate shaders, set uniform variables, draw the geometry &amp;hellip; a lot of things to do, and you can&amp;rsquo;t really understand it bit by bit. The book solves &amp;ldquo;chinken and egg&amp;rdquo; problem in very elegant way of its own utility libraries. Sample code relies on utility functions that are gradually replaced/extended with newly introduced functionality.&lt;/p&gt;

&lt;p&gt;OpenGL SuperBible is not about complex rendering algorithms and shader authoring - every sample is meant to illustrate specific API functionality. At the same time samples are not artificial, which is a big plus (i.e. there is clear example of why you might want to to multisample resolve in the shader in case of HDR rendering). Also, this book is not going to teach anyone about inner workings of modern GPUs. I think that clear focus on just teaching the API is the right thing. Considering the target of this book (beginners) it&amp;rsquo;s also really hard not to forgive that math part isn&amp;rsquo;t very thorough :) For me it was a very good read simply to catch up with things, but I also think this book is very good place to start adventure with graphics programming, and then continue education with more advanced resources. Even then, this book will probably remain a valuable reference that you&amp;rsquo;ll keep handy :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is the era of dedicated gaming devices coming to an end?</title>
      <link>https://www.maciejmroz.com/2012/02/22/is-the-era-of-dedicated-gaming-devices-coming-to-an-end/</link>
      <pubDate>Wed, 22 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2012/02/22/is-the-era-of-dedicated-gaming-devices-coming-to-an-end/</guid>
      <description>&lt;p&gt;Today was the day when PlayStation Vita launched worldwide. I had the chance to play most of the launch games already during GDC Europe last August and I think that both hardware and launch lineup are impressive. But I didn&amp;rsquo;t buy PS Vita on launch. It&amp;rsquo;s probably a very sad day for Sony, bacause &amp;ldquo;PlayStation&amp;rdquo; is my religion (on Facebook, at least :) ).&lt;/p&gt;

&lt;p&gt;What happended? I faced the truth - mainstream mobile gaming is already on Android and iOS, and a lot of titles are released every day. I know that every single developer out there is working on these platforms, and they don&amp;rsquo;t mind selling the game for $1-$5. You can&amp;rsquo;t possibly deny the trends - these devices are the new player in the console market. Just a few months ago I have written (on this blog) that there&amp;rsquo;s not much danger to Vita from the iOS/Android camp. I hate to admit that, but I might have been wrong. Yes, phones and tablets might lose on overall experience, but at the same time they win hands down when it goes to price and convenience. I actually found myself playing games on both iPhone and iPad in past six months or so. And I enjoyed doing it, at least for a short while.&lt;/p&gt;

&lt;p&gt;Of course it doesn&amp;rsquo;t mean Vita is dead on arrival. It&amp;rsquo;s the best portable gaming device Sony (or anyone) could possibly make and a lot of people will buy it. But they will be the most hardcore of the hardcore gamers. Others will settle for the simple and easy solution, sometimes even being fully aware of what they are sacrificing - it all really comes down to the added value. Some cannibalization is bound to happen. Will the remaining group of true gamers be big enough to make PS Vita a viable platform for developers? I hope so. I actually hope that PS Vita will sell a lot more than PSP. If not, we&amp;rsquo;ll end up with a platform that has 2-3 good releases a year &amp;hellip; and that&amp;rsquo;s my biggest fear. You don&amp;rsquo;t buy console at launch because of the launch titles only. You buy the console because of the future games. Lots of them. And I am not sure they are coming :(&lt;/p&gt;

&lt;p&gt;As a gamer I want truly epic experiences. PS Vita has the potential to provide them, potential that might be lost because of where the game industry is going. We are going towards simpler and shallower games, towards replacing challenge with compulsion loops, and towards freemium as a dominating business model &amp;hellip; where&amp;rsquo;s &lt;strong&gt;epic&lt;/strong&gt; in that?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is HTML5 a viable platform for games?</title>
      <link>https://www.maciejmroz.com/2012/01/12/is-html5-a-viable-platform-for-games/</link>
      <pubDate>Thu, 12 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2012/01/12/is-html5-a-viable-platform-for-games/</guid>
      <description>&lt;p&gt;If you ever tried to get deeper into it than just looking at press releases about &amp;ldquo;HTML5 companies&amp;rdquo; who just received some VC funding, it is indeed very interesting question. Let&amp;rsquo;s put aside the idea of building entire business on HTML5 tools/engines. What I want to touch to today is the topic of the technology in general, and it&amp;rsquo;s comparison with possible alternatives. Important distinction here - it&amp;rsquo;s about 2D casual games. It&amp;rsquo;s also good assumption that we are targeting mobile devices. So, what are the options? Realistic choices: Native, HTML5, Adobe AIR, some hybrid of native and HTML5 (in case of mobile devices).&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s one thing that HTML5 offers and no one else can - breaking away from app stores of any kind. It is very important, but not because of the cut app store operator takes (i.e. 30% in Apple case). Anyone who is even remotely aware of the costs and complexities associated with doing payment processing will happily give away 30% and then say &amp;ldquo;thank you for doing it for me&amp;rdquo;. This obviously does not apply to large companies for scale reasons, but for small/medium sized developers it&amp;rsquo;s only a reason to be happy. There are other reasons to break away from app stores, one is obvious and one less so, but it is actually more important.&lt;/p&gt;

&lt;p&gt;The obvious reason to go through browser is freedom - there is no approval process. You just put yout game online, and that&amp;rsquo;s really all there is to it. Your app cannot be retroactively banned in case there is some sort of dispute. You even could consider going with content that&amp;rsquo;s not universally acceptable. This isn&amp;rsquo;t a big consideration for casual games, but might be an issue for other types of games. What is important is certainty that if I put my game on the Internet, people will be able to access it, period.&lt;/p&gt;

&lt;p&gt;The less obvious reason is update process. I am living in game-as-a-service world already, and that means very short update cycles. At Ganymede, we have put quite a bit of work to automate the software releases and make them as painless as possible. Five times a day? If we want to, sure. Of course, if something goes bad, we can roll back a release in single click. We&amp;rsquo;re not doing continuous deployment, but I am sure many companies already do (although admittedly it&amp;rsquo;s more popular approach for web companies). This kind of model is not compatible with any app store. Trying to plan a lot in advance is painful (&lt;a href=&#34;http://www.youtube.com/watch?v=7ulKRB9572Y&amp;amp;feature=share&#34;&gt;and just plain bad&lt;/a&gt; [www.youtube.com]), and becomes unmanageable when you go beyond single platform. I will not say it&amp;rsquo;s impossible, because it can be done, but I already feel sorry for those who will go that way. I rarely see the requirement for frequent updates being brought up. Probably it&amp;rsquo;s because when you run into person coming from web world they just think &amp;ldquo;it&amp;rsquo;s supposed to be that way&amp;rdquo; (in a well run web shop it is almost always a given). On the other hand, traditional game developers are used to development model where launch is the end, not the beginning. Anyway, in perfect world online games use processes that are a lot closer to web development. That&amp;rsquo;s huge advantage on the side of HTML5 and one that shouldn&amp;rsquo;t be underestimated. Not going for frequest post launch updates with new content every week, optimization of user experience with A/B testing, etc means simply leaving tons of money on table. So why isn&amp;rsquo;t everyone and their grandma already writing HTML5 games for mobile?&lt;/p&gt;

&lt;p&gt;Because everything else about it just plainly sucks. Forget about &amp;ldquo;write once, run anywhere&amp;rdquo;. You&amp;rsquo;ll have to do a lot of per-device and per-browser adjustments. Performance will be far from predictable. Typically, it will be very bad. Writing your game directly in JavaScript might not be good idea productivity-wise. There are projects that compile higher level stuff into JS (i.e. &lt;a href=&#34;http://www.dartlang.org/&#34;&gt;Dart&lt;/a&gt; [www.dartlang.org] or &lt;a href=&#34;http://coffeescript.org/&#34;&gt;CoffeeScript&lt;/a&gt; [coffeescript.org]), but are you brave enough to use any of them right now? Let&amp;rsquo;s continue with the bad side: you&amp;rsquo;ll not be able to use everything the device can do, and of course your limitations will vary from device to device. I am not talking about some esoteric problems, I am talking things as basic as multitouch support or sound playback. It is that bad. Another issue that you might run into is talent alignment. To develop games, you need game developers. And very few of them are excited about web programming. Typically, they are much more likely to be excited about &lt;a href=&#34;http://developer.apple.com/library/ios/#releasenotes/General/WhatsNewIniPhoneOS/Articles/iOS5.html&#34;&gt;ARC in iOS5&lt;/a&gt; [developer.apple.com]. On the other hand, idea of web developers doing an HTML5 game is just scary, because these guys typically have no clue how to even approach doing a game (even if they are good at what they do). Now it&amp;rsquo;s time to ask: &amp;ldquo;What the hell am I supposed to do with all this mess?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;My advice is simple: don&amp;rsquo;t give in to hype. Think about all your options in the context of the project you are about to do and do not ignore what the team is already strong at (i.e. short update cycle is useless if you do not know how to take advantage of it). Also, it&amp;rsquo;s good idea to apply this thinking in the context of your current product roadmap and overall strategy, but do not spend too much time doing it. The world is changing fast, be prepared.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some thoughts on the mobile revolution</title>
      <link>https://www.maciejmroz.com/2012/01/07/some-thoughts-on-the-mobile-revolution/</link>
      <pubDate>Sat, 07 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2012/01/07/some-thoughts-on-the-mobile-revolution/</guid>
      <description>&lt;p&gt;People keep talking about &amp;ldquo;mobile&amp;rdquo;, how it&amp;rsquo;s going to be very important (i.e. &amp;ldquo;we have &lt;strong&gt;mobile first&lt;/strong&gt; policy&amp;rdquo; &amp;hellip;) but recently it really struck me: most talking about mobile revolution have abolutely no idea. They just don&amp;rsquo;t get it. Which is a bit sad, actually, especially if you think that big media companies are still struggling to make sense of what to do with classic, web-based Internet. Mobile is a revolution can only be understood by mobile users. I remember when CEO of Bigpoint was talking on GDC Europe more than a year ago about buying everyone in his company an iPad. At that time I only thought &amp;ldquo;it&amp;rsquo;s nice&amp;rdquo;. But it was much more than that. It was an investment to create competitive advantage (and a very, very high ROI investment). There&amp;rsquo;s absolutely no way to understand smartphones, tablets and their features without actually using these things. Because it is &lt;strong&gt;that&lt;/strong&gt; disruptive.&lt;/p&gt;

&lt;p&gt;Without previously experiencing these devices, one might believe that correct way of &amp;ldquo;going mobile&amp;rdquo; is to adjust user interface to fit on smaller screen, and ship. It&amp;rsquo;s great recipe for failure. Fitting existing content and PC-like user experience into something that is completely different? It cannot be made to work. It&amp;rsquo;s about entire user experience! Look at the usage pattern of mobile devices. When I pick up the phone, I want to something &lt;strong&gt;very specific, right now&lt;/strong&gt;. More often than not it is not my primary activity, so I don&amp;rsquo;t intend to waste too much time doing it. The thinking part is already behind me, now I am at the &amp;ldquo;doing&amp;rdquo; stage. This &amp;ldquo;right now&amp;rdquo; flow starts actually even before developer has any chance to interact with the user - it happens when an application is launched. Launching a web browser, going to website, and then doing something is just taking too long. It&amp;rsquo;s one the reasons we are seeing the &amp;ldquo;applification&amp;rdquo; of Internet (which is very interesting trend on its own, and it&amp;rsquo;s the very reason Google cannot afford to fail with Android).&lt;/p&gt;

&lt;p&gt;&amp;ldquo;There&amp;rsquo;s an app for that&amp;rdquo; is more than just a slogan - it&amp;rsquo;s the very definition of what is happening and what is expected. If you are interested about other driving forces behind the switch to dedicated applications, see &lt;a href=&#34;http://www.youtube.com/watch?v=BiYNs5uPPEE&#34;&gt;this video&lt;/a&gt; [youtube.com]. I have said it a few times (although not on the Internet) when talking about usability of social games: treat all your users as if they have ADHD. If an user has to wonder &amp;ldquo;what next?&amp;rdquo; it is the user you just lost. This is even more true on mobile. Not only for games, but for everything the users might possibly want to do. Their time is limited. Everything that saves time is good, everything that wastes time is bad. Simple, isn&amp;rsquo;t it? Yes, but it&amp;rsquo;s impossible to achieve if you are not obsessed about usability. Give him/her an app to do just one thing, and make it as easy and user friendly as possible. &lt;strong&gt;Nothing&lt;/strong&gt; more. If you don&amp;rsquo;t, you will lose that user, because someone else will do it the right way. This is why Facebook has dedicated Messenger app - it&amp;rsquo;s one app to do just one thing. If Facebook didn&amp;rsquo;t do this app, somebody else would. Feature creep is simply something you cannot afford. While most of the industries get away with one or two extra features without much harm, in mobile, if you are not focused, you are dead. There is no &amp;ldquo;one app to rule them all&amp;rdquo; and there will never be one. But it is still a viable strategy to go mobile with unique content and make money this way, correct? Yes and no.&lt;/p&gt;

&lt;p&gt;Funnily, answer to that doesn&amp;rsquo;t have much to do with mobile computing, more with digital age in general. The problem is simple: content (any content!) is becoming a commodity, because it grows faster than our ability to consume it, and it never disappears. This makes competing on content increasingly difficult - because you have to compete on quality, which in long term leads to a dead end. It applies to music, movies, games, stock photos, etc etc. This is just another disruption that is happening in parallell. Let&amp;rsquo;s go back to mobile devices. Is there a way to succeed there? Of course there is, but the answer is not what everyone would like to hear. Because we are talking about typical disruption as defined in the &lt;a href=&#34;http://www.amazon.com/Innovators-Dilemma-Technologies-Cause-ebook/dp/B004OC07GM/ref=sr_1_4?ie=UTF8&amp;amp;qid=1325951575&amp;amp;sr=8-4&#34;&gt;Innovator&amp;rsquo;s Dillemma&lt;/a&gt; &lt;a href=&#34;actually, more than one at the same time&#34;&gt;amazon.com&lt;/a&gt; what goes below has to be a guessing game. I might put some references to gaming industry, because that&amp;rsquo;s what I do, but I believe these guesses/conclusions to be quite general.&lt;/p&gt;

&lt;p&gt;First, traditional business models are most likely not going to work, at least not long-term. Freemium is probably the way to go but I wouldn&amp;rsquo;t be suprised to see it evolve into something else. It&amp;rsquo;s already visible in the app stores - statistically paid iOS games are losing money. This is not going to change, and it&amp;rsquo;s happening not because of piracy, but because apps are a commodity. I see a potential for true virtual economy somewhere down the line, but trying to predict how it&amp;rsquo;s going to look like is going into science fiction area.&lt;/p&gt;

&lt;p&gt;Second, there&amp;rsquo;s a lot of value in being cross platform and syncing everything &amp;ldquo;in the cloud&amp;rdquo;. You can&amp;rsquo;t just make iOS app. You may (correctly, at the time I am writing this post) believe that iOS devices are the best, but it doesn&amp;rsquo;t really matter. You have to have Android version, and not only Android version - there&amp;rsquo;s also Windows Phone, and two major desktop operating systems. Yes, desktops will be transformed, too. Think for a while what is the reason for success of Evernote or DropBox. If there is collaboration between people, there&amp;rsquo;s no guarantee of homogeneous platform. For some services it&amp;rsquo;s not about losing part of the market - it&amp;rsquo;s about losing the entire market. This is also the reason why mobile cannot be simply ignored.&lt;/p&gt;

&lt;p&gt;Advertising will be transformed, but it will not happen very soon. I see a lot of change here, but in a bit longer term. Advertising industry in general is very static. That has a lot to do with how budgets are allocated and how results are measured. In short term, we&amp;rsquo;ll see ad-supported apps as a viable and popular business model - used instead, or in connection with freemium. That brings us to the next point.&lt;/p&gt;

&lt;p&gt;User engagement is the absolute king. I know I just made a full circle, but it&amp;rsquo;s very important: people have to want to use whatever is offered to them. It&amp;rsquo;s not enough that they try it - they have to like it enough to go back, and refer it to their friends. And if we add the shift in business models it becomes obvious that in order to succeed the change of thinking is required. Instead of &amp;ldquo;How we are going to make money on it?&amp;rdquo; it&amp;rsquo;s time to start asking &amp;ldquo;If we do this project, is it making people lives better?&amp;rdquo;. Thinking outside the box matters more than ever.&lt;/p&gt;

&lt;p&gt;There will be a lot of failures, and a lot of success stories. It will be very hard, if not impossible, to predict which one is which before actually building it and trying it out with the users. There&amp;rsquo;s absolutely no point in market research because there&amp;rsquo;s no market to research - it doesn&amp;rsquo;t exist yet. Most important of all, as it progresses, mobile revolution will be even more &lt;strong&gt;exciting&lt;/strong&gt; :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why PC sales are slowing down</title>
      <link>https://www.maciejmroz.com/2011/12/03/why-pc-sales-are-slowing-down/</link>
      <pubDate>Sat, 03 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2011/12/03/why-pc-sales-are-slowing-down/</guid>
      <description>&lt;p&gt;There has been a lot of talk lately about how tablets are eating into the classic PC market, that people don&amp;rsquo;t need PCs any more, and how doomed they are. It&amp;rsquo;s true that we are witnessing a decline of the PC platform, and that this trend will only intensify. At the same time I don&amp;rsquo;t believe that &lt;strong&gt;right now&lt;/strong&gt; it has much to do with rapid growth of tablet market. There are many other resons it is bad to be a PC vendor right now.&lt;/p&gt;

&lt;p&gt;First of all, for quite some time now desktop computers are simply &lt;strong&gt;good enough&lt;/strong&gt;. There used to be the push to buy newer and faster hardware every year but for most people that need is gone. Many use their machines until they break down, and not until they can buy something faster. What activities really require a lot of processing power? And even more interesting question - how many of these activities will not migrate to the cloud in near future? I don&amp;rsquo;t need faster desktop computer. I might need hardware that&amp;rsquo;s instantly on, consumes less power, is quiet, and syncs to every other device I have, but faster it not among my requirements for a PC. Down the line, I don&amp;rsquo;t really need more storage - days of &amp;ldquo;downloading the internet&amp;rdquo; are long gone.&lt;/p&gt;

&lt;p&gt;What used to differentiate PCs is now just a set of meaningless numbers. The cheapest one you can get will be good enough in terms of performance and storage. If you add the fact that modern electronics is fairly durable, you get lenghtening of average upgrade cycle, and in consequence - declining demand. Combine that with low margins and it&amp;rsquo;s obvious why HP wants out &amp;hellip;&lt;/p&gt;

&lt;p&gt;It gets worse. People actually started buying slower hardware, like netbooks and tablets, because it suits their usage patterns better - to the point whese sacrificing performance for size and portability makes sense. A trend that really has hit Microsoft hard - every operating system they built so far had bigger requirements than previous one. Windows 8 reverses this trend - it&amp;rsquo;s going to consume less resources for than Windows 7. Great for Microsoft because they understand &amp;ldquo;adapt or die&amp;rdquo; philosopy, very bad for PC vendors because release of next version of Windows no longer drives next hardware upgrade cycle. Demand for new PCs declines even more.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not that much better in corporate world - yes, we will always need more processing power, but not on desktops. Our processing needs will be on servers. Nobody is doing complex data analysis on the desktop any more. We also do less and less of software builds on the desktops, too. Even small teams are using continuous integration servers today, very often doing builds and automated testing in distributed way. Again, for many people desktop is becoming just a tool to launch browser/simplistic software, and not where real processing happens. Commodity hardware is also more often than not enough to use content creation software - so why upgrade?&lt;/p&gt;

&lt;p&gt;Tablets are something that&amp;rsquo;s only starting to happen on top of these trends. As hardware and software matures we indeed might see people choose not to buy another deskop PC, and for some tablet will become the only computing device they use. We are still early on adoption curve, though. The year 2012 might just be the year when we see start of mass market adoption of these devices. The prices will drop for both hardware and services like mobile broadband, and that will eventually move tablet from luxury into commodity.&lt;/p&gt;

&lt;p&gt;PC vendors are doomed because of all these trends, not just one of them. They are also facing &amp;ldquo;adapt of die&amp;rdquo; ultimatum. They just can&amp;rsquo;t be winners without radical shifts in their business. On the other hand, there are great opportunities related to these trends: networking solutions, datacenter management, cloud services and middleware, data storage solutions, etc &amp;hellip; that&amp;rsquo;s &lt;strong&gt;bound to explode&lt;/strong&gt;. Great place to be in when starting a company today, by the way :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is it time to try an Adroid phone now?</title>
      <link>https://www.maciejmroz.com/2011/05/08/is-it-time-to-try-an-adroid-phone-now/</link>
      <pubDate>Sun, 08 May 2011 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2011/05/08/is-it-time-to-try-an-adroid-phone-now/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been an IPhone user for loong time. While I did not own the &amp;ldquo;1G&amp;rdquo;, I have my 3G for more than two years now. And, to be honest, it begins to show its age. For quite some time, it wasn&amp;rsquo;t getting any of the new cool stuff in iOS, and iOS version 4 actually should have never been released on iPhone 3G because it makes a lot of stuff run noticeably slower. So, the question is whether to upgrade to iPhone 4.&lt;/p&gt;

&lt;p&gt;The biggest advantages of the iPhone right now are: retina display and iOS (again, I believe that it is the best mobile system out there right now). But the rest of the world wasn&amp;rsquo;t in standstill during the past two years. There are Android based phones that at least on paper look better than iPhone 4 (except for the display, nothing out there can really match it) - fast dual core CPU, more memory, better GPU, better camera, comparable internal flash memory, Flash Player support, NFC chip &amp;hellip; iPhone 4 has none of these, and is more expensive. It also is a very safe choice and an upgrade in every way possible over the iPhone 3G I am using right now.&lt;/p&gt;

&lt;p&gt;The truth is, I am very &lt;strong&gt;used to&lt;/strong&gt; iOS. I just like it. At the same time I know that buying iPhone 4 right now might not be the best choice. Waiting for iPhone 5 is an option (it will surely feature A5 chip), but seeing the paper-only availability of iPad 2, it&amp;rsquo;s not very optimistic wait. Even assuming that paper launch will be in June, real availability will be probably in September, and possibly later here in Poland. And that&amp;rsquo;s assuming Apple would actually launch it in June. By the way, iPad 2 is officially available for more than a month right now in Poland - but good luck buying one at official Apple reseller. They still have waiting list on the damn thing :(&lt;/p&gt;

&lt;p&gt;So, in case of a phone, there&amp;rsquo;s strong possibility of defecting towards Android camp. Not because I want to, but because Apple might not deliver on time. In case I would go for an Adroid device, the new problem arises. Which one?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>There is no such thing as best computing device</title>
      <link>https://www.maciejmroz.com/2011/03/06/the-lines-are-really-getting-blurry/</link>
      <pubDate>Sun, 06 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2011/03/06/the-lines-are-really-getting-blurry/</guid>
      <description>&lt;p&gt;The recent releases from Apple (iPad 2 and earlier new laptops) get me thinking a bit about evolution of computing devices and how we actually use them. The past was very simple: loud and powerful desktops, underpowered laptops with short battery life, and feature phones that couldn&amp;rsquo;t really do much. The technology progress changed things a lot.&lt;/p&gt;

&lt;p&gt;As desktops and laptops countinued to improve, smartphones were introduced to the world. It&amp;rsquo;s hard to tell when that happened, but at some point they could do quite a bit. Then we had netbooks, ultraportable laptops (like MBA) and finally tablet devices. There&amp;rsquo;s huge overlap in hardware capabilities and possible usage between all these devices. It&amp;rsquo;s going to get even more complicated in future. Apple already announced a bunch os iOS-like features in their upcoming Mac OS X Lion. Apple already has fully functional iOS emulator to use with their XCode development environment. Fast forward a year or two: why not let iOS applications run on Mac OS X machines as full screen applications? The clear cut between desktop OS and mobile OS is going to be no more.&lt;/p&gt;

&lt;p&gt;Interestingly, all this convergence is exactly what pushes the world towards &amp;ldquo;cloud based&amp;rdquo; services, or &amp;ldquo;software as a service&amp;rdquo; (which I think is a lot more accurate name) solutions. Google Mail, Google Docs, Picasa, Flickr, YouTube, Evernote, DropBox, Facebook &amp;hellip; what these have in common is ability to use from every device you see fit at the moment. Storing our movies and music on local hard drive (or SSD) in a few years will be as anachronic as CD-R discs are today. What does it mean to consumers? For a start, nobody is going to need all of these devices. The choice will mostly come down to answering the question &amp;ldquo;how it fits my lifestyle?&amp;rdquo;. Second, less obvious thing: the personal storage needs are not going to grow as fast as they used to. Most of the storage growth will be somewhere in the cloud. Last but not least, desktop PC is a dying breed, and the trend will only intensify.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why PSP2 is going to succeed</title>
      <link>https://www.maciejmroz.com/2011/01/30/why-psp2-is-going-to-succeed/</link>
      <pubDate>Sun, 30 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2011/01/30/why-psp2-is-going-to-succeed/</guid>
      <description>&lt;p&gt;They actually call it NGP, standing for Next Generation Portable. I wouldn&amp;rsquo;t really be suprised that much if the name was left unchanged in final product. Anyway, I&amp;rsquo;ve seen some posts from respected sites saying that PSP2 is doomed, dead on arrival, etc etc &amp;hellip; which I think is completely wrong thinking. The basic problem is comparing it to a phone, usually the iPhone.&lt;/p&gt;

&lt;p&gt;It is true that games are huge on Apple platform, and they are only going to get bigger. But comparing Angry Birds or Cut The Rope to God of War, Tekken or Wipeout &amp;hellip; come on, be serious here for a moment. &lt;strong&gt;It is not the same market&lt;/strong&gt;. One can actually argue that PSP (the first one) has been offering the best portable gaming experience to this day. The only real competitor it has is the Nintendo DS, which is also a &lt;strong&gt;dedicated&lt;/strong&gt; device. And being dedicated device targeted at specific audience is very important.&lt;/p&gt;

&lt;p&gt;The biggest issue is controls. Both Sony and Nintendo have built their devices for gaming, available buttons and their layout are optimized for this purpose only. You don&amp;rsquo;t do phone calls with these devices, you don&amp;rsquo;t surf the web, you don&amp;rsquo;t read your mail, or use Skype. Even if you can do some of these things, they are not the primary focus. Even if NGP has all of mentioned functionality, the device is clearly created with games in mind. Because that is what target audience cares about. Guess what phones are primarily made for? If you said &amp;ldquo;gaming&amp;rdquo; you are wrong. Yes, recent phones are more powerful than DS/PSP, but do they offer better gaming experience? Today&amp;rsquo;s smartphones are meant to do all, and be perfect at communication: phone calls, sms, mail, Twitter, Facebook &amp;hellip; games are just an addition.&lt;/p&gt;

&lt;p&gt;Let me repeat one thing to be very clear here: I absolutely believe that smartphone gaming market is going to continue growing, I just don&amp;rsquo;t see how it could seriously hurt dedicated gaming devices. It would be in my mind similar to like saying that FarmVille steals players from Call of Duty. Actually, social gaming phenomena is very good to compare here - it&amp;rsquo;s much less about stealing existing customers, and much more about creating completely new market. NGP has to appeal to its target market, and it does everything it needs to do so. Every owner of the PSP will agree that just one analog stick in first PSP is a big limitation. Fixed, NGP has dual analog sticks. And they don&amp;rsquo;t have to be emulated using touchscreen, like in many iPhone games. UMD might be a good decision at the time first PSP was released, but is had to go. Ability to have more than 1GB on single disc is great (it totally beats smartphone game size limits, even today), but as a PSP owner I&amp;rsquo;ll know one thing - having to carry all these UMD discs around is far from great. And, even after PSP Go was released, Sony offered no way to copy games into internal memory/flash card. They fixed that in NGP: no more discs, games will be distributed on flash cards. Touch screen and touch panel in the back will allow developers to use touch controls &lt;strong&gt;where it actually matters&lt;/strong&gt;. Add to this very powerful CPU/GPU pair and low level hardware access for developers.&lt;/p&gt;

&lt;p&gt;For a gamer NGP is a must buy. If Sony doesn&amp;rsquo;t do anything stupid, and release the device in sensible time, I don&amp;rsquo;t really see how it could fail.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What to expect from iPad 2</title>
      <link>https://www.maciejmroz.com/2011/01/20/what-to-expect-from-ipad-2/</link>
      <pubDate>Thu, 20 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>https://www.maciejmroz.com/2011/01/20/what-to-expect-from-ipad-2/</guid>
      <description>&lt;p&gt;Before CES, I&amp;rsquo;d say &amp;ldquo;not much&amp;rdquo;. The thing is, today Apple owns the tablet market (if defined as dedicated hardware running dedicated OS), there is almost no competition out there. But it is on the horizon. Take Motorola Xoom as an example. Hardware specs are way beyond iPad 1.0, and if Android 3.0 is going to be that good as people claim, Apple is going to be in real trouble if they release underpowered device. And there will be more and more Android tablets as we go deeper into 2011 &amp;hellip; My guesses for iPad 2 are: SD card slot, front facing camera, and 1GB of RAM. SD card slot is fairly cheap way to greatly improve the device, and is very much in line with intended use - something that would replace laptop for basic photo management tasks. Front facing camera - not much point to discuss. Memory is more interesting - a month ago I&amp;rsquo;d bet on 512 MB, exactly the same as in iPhone 4. But after CES it just sounds cheap and limited. Apple has tradition of selling as little as they can for as much as possible, but I don&amp;rsquo;t really think they can get away with that. I think this is very moderate spec that should be considered basis for more. Retina display? Dual core CPU? Faster GPU? USB slot? They need another iPhone 4, and this time without the antennagate. Right now, after CES, anything is possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Apple Tablet</title>
      <link>https://www.maciejmroz.com/2010/02/06/apple-ipad-first-thoughts/</link>
      <pubDate>Sat, 06 Feb 2010 11:52:02 +0100</pubDate>
      
      <guid>https://www.maciejmroz.com/2010/02/06/apple-ipad-first-thoughts/</guid>
      <description>&lt;p&gt;Apple tablet is no longer a mystery. The question now is whether it meets the expectations or fits any real needs :) Basically it is an oversized iPhone - 1024x768 multitouch display, 16-64 GB of flash memory, 1 GHz custom CPU, WiFi, Bluetooth etc, and it runs iPhone OS. I already have an iPhone. Is iPad going to perform better enough to justify buying additional device?&lt;/p&gt;

&lt;p&gt;GMail support with push in the iPhone is most of the time sufficient for me. I rarely reply to mails directly from my iPhone, though. If something requires longer answer I usually just use normal computer for that. iPads bigger screen (and bigger virtual keyboard as a result) will surely improve experience here. I can actually imagine writing emails using this thing. Web browsing is where iPad is going to be a lot better than iPhone simply because of screen size and CPU performance (one of the things I noticed immediately at Apple videos is how smooth it runs, it&amp;rsquo;s super fast!). Web and mail if work well might alone be a selling point for people who don&amp;rsquo;t keep &lt;sup&gt;24&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt; computer at home and actually do turn it on and off. In that scenario iPad is going to be very useful, offering browsing and mail experience very close to &amp;ldquo;full&amp;rdquo; but with instant access.&lt;/p&gt;

&lt;p&gt;iTunes for music/video/podcasts is nice for people living in the US, I imagine. Apple still has not fully acknowledged existence of Poland, so for me this functionality is not very attractive, simply because it does not work. Of course I can transcode videos and use MP3 for music but that&amp;rsquo;s hardly the experience it should be. For watching video I don&amp;rsquo;t think it can realistically beat big LCD TV, I know that there are some VGA/composite adapters (sold separately), but what I&amp;rsquo;d really like to see is just HDMI output. Or perhaps built in DLNA media server? Both would solve the problem of &amp;ldquo;how to put it on large screen&amp;rdquo;, second one is wireless, which in my dictionary is almost the same as &amp;ldquo;better&amp;rdquo; :)&lt;/p&gt;

&lt;p&gt;Photo functionality is nice but crippled at the very beginning by the hardware - iPad does not have SD card slot nor USB port. What may be the most attractive usage is loading the photos directly from my camera into the iPad, tag them, rate, sort, delete ones I don&amp;rsquo;t like, upload to Facebook etc. It will be possible but will require additional &amp;ldquo;camera connection kit&amp;rdquo;. Not including these ports by default it strange to say the least. Perhaps they are already planning revised hardware version late this year, and that&amp;rsquo;s just sinister plan to make people buy new hardware in longer term :). Anyway, I really do see the potential here.&lt;/p&gt;

&lt;p&gt;iBookstore was obviously going to happen - and I think it is very good it did. It will put a lot of pressure on ebook reader market. I am not entirely convinced on using active IPS display for reading, in the end it is not that much different from staring at the lamp for a long time. Theoretically e-ink displays have an advantage here, and if you try using iRex Iliad (for example) then yes, its display technology is awesome in terms of ease of reading. At the same time, everything else is far from great. Iliad is slow, takes ages to start up, and again ages to load large pdf files, page turning is sluggish, battery life is not so great, and overall user interface is far from easy to navigate. If Apple really can get 10 hours of battery life from iPad (and one month of standby!), while running active high resolution color display, it clearly shows that there is something wrong with all ebook readers released to market so far. This really reminds me of what Apple did to cell phone market. Perhaps they did not built perfect device but still managed to do something better than everyone else. Again, I see a lot of potential here. I&amp;rsquo;d really like to buy books in electronic rather than physical form, but I guess there&amp;rsquo;s long time before that really happens. What is needed is immediate availability of electronic version along with print release, and availability of electronic version of every single book out there. None of these is happening today, probably not even in the US. We&amp;rsquo;ll get there in a few years. Printing industry is insanely wasteful and while I do not care about the environment that much in general, when I can do something environmentally friendly and convenient at the same time - I am all for it. For now, I only want to know if I can upload pdf files directly to the iPad - I might not be able to use it to read commercial books, but there surely is tons of technical documentation, presentations etc that I might want to read on the iPad.&lt;/p&gt;

&lt;p&gt;One glaring omission I think everyone considered almost a given before iPad announcement is front facing camera. It means no video conferencing. Perhaps it has something to do with unlimited 3G data plans Apple and AT&amp;T; are offering in the US? Anyway I do see it as a shortcoming, and something surely to be corrected in future hardware release (and everyone will go and buy themselves a new iPad &amp;hellip; Apple is thinking long term here :) ). Network connectivity is where it manages to do very well. 802.11n + Bluetooth 2.1 is very good, built in 3G modem is also nice. There&amp;rsquo;s nothing more to want here. I&amp;rsquo;d only like to know if I can use iPhone as Bluetooth modem for non-3G iPad. The price difference between 3G and non-3G versions is significant and I can&amp;rsquo;t really imagine scenario when I am mobile and without my phone. So why two SIM cards? Seems logical that in this case I should simply be able to go with WiFi-only version.&lt;/p&gt;

&lt;p&gt;Apple did not forget about productivity apps, and I am sure there will be more with time. However this is where you really need multitasking. iWork applications are ok, but in reality you are switching between these apps and email/web/skype/calculator/pdf reader etc. Non to mention listening to music in background. Which means we are left with very basic functionality. That can be fixed with OS update, at the cost of battery life (which Apple seems to be defending very aggressively). In the end, iPad is no replacement for a laptop or PC/Mac, although to be fair I have to say that it does not try to be one.&lt;/p&gt;

&lt;p&gt;The idea of doing simple tasks on simple device leads me to another question: do people still need a PC/laptop? Think about it: typical people mostly consume media, what they produce (outside of work) is rather simplistic: photos, videos, some blog posts. They also need basic calendar/todo list, perhaps money management application. Add to this the trend of moving more and more online and running it inside a web browser (obviously a model supported by iPad) and we are soon approaching the world where all of typical stuff people do can be done using one simple device, probably looking a lot like the iPad. I believe it indeed is a new device category, that takes away some of the tasks we so far are used to do on personal computer. I see very strong analogy to game consoles here, btw. I do not play typical &amp;ldquo;3D&amp;rdquo; games on my computer any more. Sure I&amp;rsquo;ll use it to play Civilization, but for most games the console is just better. It&amp;rsquo;s likely going to be the same with iPad, but this time what&amp;rsquo;s left of the personal computer may no longer be necessary for many people.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>